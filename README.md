# ğŸ§  Automated Drive-Thru Assistant

This project presents a comprehensive offline, AI-driven drive-thru ordering system, integrating real-time facial recognition, automated speech processing, and natural language understanding for a fully autonomous customer experience.

---

## ğŸš€ Key Capabilities

- ğŸ¥ **Facial Detection and Recognition** â€” Leverages OpenCV-based algorithms to identify and greet users personally, facilitating user-specific interactions.
- ğŸ˜¤ **Offline Automatic Speech Recognition (ASR)** â€” Employs [Vosk](https://github.com/alphacep/vosk-api) for low-latency speech transcription independent of internet connectivity.
- ğŸ’¬ **Local LLM-Based Semantic Parsing** â€” Utilizes [Mistral](https://ollama.com/) via Ollama to convert natural language input into structured, JSON-based order intents.
- ğŸ—ï¸ **Contextual Multi-Turn Dialogue Management** â€” Enables real-time updates, cancellations, and confirmations across a sustained conversational session.
- ğŸ“¢ **Synthetic Speech Output** â€” Delivers dynamic verbal feedback through `pyttsx3` to simulate human-agent interaction.
- ğŸ–¼ï¸ **GUI with PyQt5** â€” Presents a full-screen user interface comprising a dynamic menu display, live video feed with annotations, a transcription window, and session status bar.
- ğŸ’µ **Dynamic Menu Integration and Pricing Computation** â€” Ingests a structured JSON menu, displaying categorized items with images and calculating real-time order totals.
- â™»ï¸ **Autonomous Session Lifecycle Management** â€” Automatically resets the interface and state following user departure, based on persistent absence detection.

---

## ğŸ“¸ Demonstration

> _Coming soon_: A walkthrough video or animated showcase.

---

## ğŸ“‚ Directory Layout

```
Automated_Drive_Thru/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ audio/           # Modules for speech recognition and synthesis
â”‚   â”œâ”€â”€ data/            # Structured configuration and content data
â”‚   â”œâ”€â”€ interface/       # GUI definition using PyQt5
â”‚   â”œâ”€â”€ nlp/             # LLM interface and query abstraction
â”‚   â”œâ”€â”€ order/           # Order modeling and management logic
â”‚   â”œâ”€â”€ utils/           # Utility scripts and helpers
â”‚   â”œâ”€â”€ assets/          # Visual resources (e.g., menu item images)
â”‚   â””â”€â”€ vision/          # Face detection and recognition layers
â”‚
â”œâ”€â”€ known_faces/         # Persisted user facial embeddings
â”œâ”€â”€ models/              # Local speech recognition model files
â”œâ”€â”€ main.py              # Application entry point
â””â”€â”€ requirements.txt     # Python dependency list
```

---

## ğŸ› ï¸ System Requirements

- Python 3.10 or newer
- [Ollama](https://ollama.com) for local LLM deployment
- Package installation:

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Core Models Employed

- **Facial Recognition**: OpenCV LBPH algorithm trained on local face samples
- **ASR**: Vosk Small English model (fully offline)
- **Language Model**: Mistral via Ollama
- **TTS Engine**: pyttsx3 for platform-native speech synthesis

---

## ğŸ“‚ Execution Guide

```bash
# Step 1: Initialize local LLM
ollama run mistral

# Step 2: Launch the application
python main.py
```

> Ensure the appropriate Vosk model is located in the `models/` directory and `menu.json` is well-formed.

---

## ğŸ“Œ Potential Extensions

- ğŸ—ï¸ Persist user receipts as local files
- ğŸ§  Introduce order recommendation based on historical trends
- ğŸŒ Develop web-based interface with FastAPI and WebRTC
- ğŸ§ª Incorporate modular testing suites for key components

---

## ğŸ“„ License

This project is licensed under the **Creative Commons Attribution-NonCommercial 4.0 International License**.  
You may view and use this code for personal or educational purposes, but **commercial use is strictly prohibited**.